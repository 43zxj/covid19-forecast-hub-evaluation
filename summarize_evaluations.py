"""Summarize individual evaluations generated by evaluate_models.py

COVID-19 Forecast Hub: https://github.com/reichlab/covid19-forecast-hub
Learn more at: https://github.com/youyanggu/covid19-forecast-hub-evaluation

To see list of command line options: `python summarize_evaluations.py --help`
"""

import argparse
import datetime
import glob
import os
from pathlib import Path

import pandas as pd


def str_to_date(date_str, fmt='%Y-%m-%d'):
    """Convert string date to datetime object."""
    return datetime.datetime.strptime(date_str, fmt).date()


def get_dates_from_fname(fname):
    """Returns the projection and eval date given a file name."""
    basename = os.path.basename(fname)
    proj_date = str_to_date(basename.split('_')[0])
    eval_date = str_to_date(basename.split('_')[1])
    assert eval_date > proj_date

    return proj_date, eval_date


def filter_fnames_by_weeks_ahead(fnames, weeks_ahead):
    """Return evaluation files that match the provided weeks_ahead."""
    include_fnames = []
    for fname in fnames:
        proj_date, eval_date = get_dates_from_fname(fname)

        days_ahead = (eval_date - proj_date).days
        max_days_tolerance = 3
        if abs(days_ahead - 7*weeks_ahead) <= max_days_tolerance:
            # this is a weeks_ahead forecast
            include_fnames.append(fname)

    return include_fnames


def main(eval_date, weeks_ahead, evaluations_dir, out_dir):
    """We combine various evaluations based on either evaluation date or weeks ahead.

    For full description of methods, refer to:
    https://github.com/youyanggu/covid19-forecast-hub-evaluation
    """
    print('Evaluation date:', eval_date)
    print('Weeks ahead:', weeks_ahead)
    print('Evaluations dir:', evaluations_dir)
    print('Output dir:', out_dir)

    assert eval_date or weeks_ahead, \
        'must provide either an --eval_date or --weeks_ahead'
    assert not (eval_date and weeks_ahead), \
        'must provide only one of --eval_date or --weeks_ahead'

    print('==============================')
    print('US evaluations')
    print('==============================')
    if eval_date:
        us_errs_fnames = sorted(glob.glob(
            f'{evaluations_dir}/{eval_date}/*_{eval_date}_us_errs.csv'))
    else:
        us_errs_fnames = sorted(glob.glob(
            f'{evaluations_dir}/*/*_us_errs.csv'))
        us_errs_fnames = filter_fnames_by_weeks_ahead(us_errs_fnames, weeks_ahead)

    assert len(us_errs_fnames) > 0, 'Need US evaluation files'

    col_to_data_us = {}
    for us_errs_fname in us_errs_fnames:
        proj_date_, eval_date_ = get_dates_from_fname(us_errs_fname)
        df_us = pd.read_csv(us_errs_fname, index_col=0)
        df_us['perc_error'] = df_us['perc_error'].str.rstrip('%').astype('float') / 100

        col_to_data_us[f'perc_error_{proj_date_}_{eval_date_}'] = df_us['perc_error']

    df_all_us = pd.DataFrame(col_to_data_us)
    df_all_us = df_all_us.dropna(how='all')
    df_all_us = df_all_us.reindex(sorted(df_all_us.columns), axis=1)

    max_rank_us = df_all_us.abs().rank().max()+1
    cols_for_ranking_us = [c for c in df_all_us.columns if 'perc_error' in c]
    df_all_us = df_all_us.reindex(df_all_us.abs().rank().fillna(
        max_rank_us)[cols_for_ranking_us].mean(axis=1).sort_values().index)

    print('US errors:')
    print(df_all_us[cols_for_ranking_us])
    print('US rankings:')
    print(df_all_us[cols_for_ranking_us].abs().rank())

    if out_dir:
        if eval_date:
            out_fname_us = f'{out_dir}/summary_us_{eval_date}.csv'
        else:
            out_fname_us = f'{out_dir}/summary_{weeks_ahead}_weeks_ahead_us.csv'
        df_all_us.to_csv(out_fname_us, float_format='%.3f')
        print('Saved states summary to:', out_fname_us)

    print('==============================')
    print('State-by-state evaluations')
    print('==============================')
    if eval_date:
        states_errs_fnames = sorted(glob.glob(
            f'{evaluations_dir}/{eval_date}/*_{eval_date}_states_abs_errs.csv'))
    else:
        states_errs_fnames = sorted(glob.glob(
            f'{evaluations_dir}/*/*_states_abs_errs.csv'))
        states_errs_fnames = filter_fnames_by_weeks_ahead(states_errs_fnames, weeks_ahead)

    assert len(states_errs_fnames) > 0, 'Need state-by-state evaluation files'

    col_to_data_states = {}
    for states_errs_fname in states_errs_fnames:
        proj_date_, eval_date_ = get_dates_from_fname(states_errs_fname)
        df_states = pd.read_csv(states_errs_fname, index_col=0)

        col_to_data_states[f'mean_abs_error_{proj_date_}_{eval_date_}'] = df_states['mean']
        col_to_data_states[f'median_abs_error_{proj_date_}_{eval_date_}'] = df_states['median']

    df_all_states = pd.DataFrame(col_to_data_states)
    df_all_states = df_all_states.dropna(how='all')
    df_all_states = df_all_states.reindex(sorted(df_all_states.columns), axis=1)

    max_ranks_states = df_all_states.abs().rank().max()+1
    cols_for_ranking_states = [c for c in df_all_states.columns if 'mean_abs_error' in c]
    df_all_states = df_all_states.reindex(df_all_states.abs().rank().fillna(
        max_ranks_states)[cols_for_ranking_states].mean(axis=1).sort_values().index)

    print('State-by-state errors:')
    print(df_all_states[cols_for_ranking_states])
    print('State-by-state rankings:')
    print(df_all_states[cols_for_ranking_states].abs().rank())

    if out_dir:
        if eval_date:
            out_fname_states = f'{out_dir}/summary_states_{eval_date}.csv'
        else:
            out_fname_states = f'{out_dir}/summary_{weeks_ahead}_weeks_ahead_states.csv'
        df_all_states.to_csv(out_fname_states, float_format='%.1f')
        print('Saved states summary to:', out_fname_states)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description=('Given an evaluation date or weeks ahead (not both),'
            'summarize all the historical projections that fit the criteria.'))
    parser.add_argument('--eval_date',
        help='Evaluate all projections based on eval_date')
    parser.add_argument('--weeks_ahead', type=int,
        help='Evaluate all projections based on number of weeks ahead.')
    parser.add_argument('--evaluations_dir',
        help='Directory containing the raw evaluations.')
    parser.add_argument('--out_dir',
        help='Directory to save output data.')

    args = parser.parse_args()
    eval_date = args.eval_date
    weeks_ahead = args.weeks_ahead
    evaluations_dir = args.evaluations_dir
    out_dir = args.out_dir

    if eval_date:
        eval_date = str_to_date(args.eval_date)
    if not evaluations_dir:
        evaluations_dir = Path(__file__).parent / 'evaluations'

    main(eval_date, weeks_ahead, evaluations_dir, out_dir)
    print('Done', datetime.datetime.now())

